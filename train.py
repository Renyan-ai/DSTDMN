import pandas as pd
import argparse
import time
import util
from util import *
import random
from model import DSTDMN
from ranger import Ranger
from logger_manager import setup_logger
from torch.optim.lr_scheduler import ReduceLROnPlateau
import os
import torch
import numpy as np
import signal
import sys

# -------------------------------

parser = argparse.ArgumentParser()
parser.add_argument("--device", type=str, default="cuda:1", help="")
parser.add_argument("--data", type=str, default="PEMS07", help="data path")
parser.add_argument("--input_dim", type=int, default=3, help="input_dim")
parser.add_argument("--channels", type=int, default=128, help="number of nodes")
parser.add_argument("--num_nodes", type=int, default=307, help="number of nodes")
parser.add_argument("--input_len", type=int, default=12, help="input_len")
parser.add_argument("--output_len", type=int, default=12, help="out_len")
parser.add_argument("--batch_size", type=int, default=64, help="batch size")
parser.add_argument("--learning_rate", type=float, default=0.001, help="learning rate")
parser.add_argument("--dropout", type=float, default=0.1, help="dropout rate")
parser.add_argument(
    "--weight_decay", type=float, default=0.0001, help="weight decay rate"
)
parser.add_argument("--epochs", type=int, default=1000, help="")
parser.add_argument("--print_every", type=int, default=50, help="")
parser.add_argument(
    "--save",
    type=str,
    default="./logs",
    help="save path (root log directory)",
)
parser.add_argument(
    "--experiment_name",
    type=str,
    default=None,
    help="experiment name (if None, use timestamp)",
)
parser.add_argument(
    "--log_level",
    type=str,
    default="INFO",
    help="logging level (DEBUG, INFO, WARNING, ERROR)",
)
parser.add_argument(
    "--auto_clean_days",
    type=int,
    default=30,
    help="automatically archive logs older than this many days",
)
parser.add_argument(
    "--es_patience",
    type=int,
    default=100,
    help="quit if no improvement after this many iterations",
)
args = parser.parse_args()


class trainer:
    def __init__(self, scaler, input_dim, channels, num_nodes, input_len, output_len, dropout, lrate, wdecay, device):
        self.model = DSTDMN(device, input_dim, channels, num_nodes, input_len, output_len, dropout)
        self.model.to(device)
        self.optimizer = Ranger(self.model.parameters(), lr=lrate, weight_decay=wdecay)
        self.loss = util.MAE_torch
        self.scaler = scaler
        self.clip = 5
        print("The number of parameters: {}".format(self.model.param_num()))
        print(self.model)

    def load_checkpoint(self, checkpoint_path, logger):
        if not os.path.exists(checkpoint_path):
            return False, 1, float('inf'), float('inf'), 0
        logger.info(f"æ­£åœ¨ä»æ£€æŸ¥ç‚¹ {checkpoint_path} åŠ è½½çŠ¶æ€...")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=next(self.model.parameters()).device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch']
            best_valid_loss = checkpoint.get('best_valid_loss', float('inf'))
            best_test_mae = checkpoint.get('best_test_mae', float('inf'))
            epochs_since_best_mae = checkpoint.get('epochs_since_best_mae', 0)
            logger.info(f"ç»§ç»­ä» Epoch {start_epoch + 1} å¼€å§‹è®­ç»ƒã€‚")
            logger.info(f"ä¸Šæ¬¡æœ€ä½³éªŒè¯æŸå¤±: {best_valid_loss:.4f}ï¼Œä¸Šæ¬¡æœ€ä½³æµ‹è¯• MAE: {best_test_mae:.4f}")
            return True, start_epoch + 1, best_valid_loss, best_test_mae, epochs_since_best_mae
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}ã€‚å°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
            return False, 1, float('inf'), float('inf'), 0

    def train(self, input, real_val):
        self.model.train()
        self.optimizer.zero_grad()
        output = self.model(input)
        output = output.transpose(1, 3)
        real = torch.unsqueeze(real_val, dim=1)
        predict = self.scaler.inverse_transform(output)
        loss = self.loss(predict, real, 0.0)
        loss.backward()
        if self.clip is not None:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)
        self.optimizer.step()
        mape = util.MAPE_torch(predict, real, 0.0).item()
        rmse = util.RMSE_torch(predict, real, 0.0).item()
        wmape = util.WMAPE_torch(predict, real, 0.0).item()
        return loss.item(), mape, rmse, wmape

    def eval(self, input, real_val):
        self.model.eval()
        output = self.model(input)
        output = output.transpose(1, 3)
        real = torch.unsqueeze(real_val, dim=1)
        predict = self.scaler.inverse_transform(output)
        loss = self.loss(predict, real, 0.0)
        mape = util.MAPE_torch(predict, real, 0.0).item()
        rmse = util.RMSE_torch(predict, real, 0.0).item()
        wmape = util.WMAPE_torch(predict, real, 0.0).item()
        return loss.item(), mape, rmse, wmape

def seed_it(seed):
    random.seed(seed)
    os.environ["PYTHONSEED"] = str(seed)
    np.random.seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.enabled = True
    torch.manual_seed(seed)



def save_checkpoint(epoch, model, optimizer, scheduler, valid_loss, test_mae, epochs_since_best_mae, checkpoint_path, logger):
    try:
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_valid_loss': valid_loss,
            'best_test_mae': test_mae,
            'epochs_since_best_mae': epochs_since_best_mae
        }
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {checkpoint_path}ï¼Œå½“å‰ Epoch: {epoch}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")


# -----------------------------


def main():
    seed_it(42)

    data = args.data

    # ç”Ÿæˆå®éªŒåç§°ï¼ˆæ•°æ®é›†åç§° + æ—¶é—´æˆ³ï¼‰
    if args.experiment_name is None:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        experiment_name = f"{data}_{timestamp}"
    else:
        experiment_name = args.experiment_name

    # åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨
    logger_mgr = setup_logger(
        log_dir=args.save,
        experiment_name=experiment_name,
        log_level=args.log_level,
        auto_clean_days=args.auto_clean_days
    )

    logger = logger_mgr.logger

    # å®šä¹‰å…¨å±€å˜é‡ï¼Œç”¨äºä¿¡å·å¤„ç†å‡½æ•°è®¿é—®å’Œä¿®æ”¹ (æ³¨æ„ï¼šåœ¨Python 3ä¸­ï¼Œä¿¡å·å¤„ç†å™¨å†…éƒ¨å¯ä»¥ç›´æ¥è®¿é—®å¤–éƒ¨ä½œç”¨åŸŸçš„å˜é‡ï¼Œä½†ä¸ºäº†æ¸…æ™°ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå®šä¹‰å®ƒä»¬)
    global current_epoch, loss, test_log, epochs_since_best_mae, engine, scheduler, checkpoint_path

    # è®­ç»ƒçŠ¶æ€åˆå§‹åŒ–ï¼ˆå°†åœ¨åŠ è½½æ£€æŸ¥ç‚¹åæ›´æ–°ï¼‰
    start_epoch = 1
    loss = 9999999  # å¯¹åº” best_valid_loss
    test_log = 999999  # å¯¹åº” best_test_mae
    epochs_since_best_mae = 0
    bestid = 0  # è®°å½•æœ€ä½³æ¨¡å‹çš„ epoch
    current_epoch = start_epoch

    name = args.data

    if args.data == "PEMS08":
        args.data = "data/" + args.data
        args.num_nodes = 170
        args.num_nodes = 207
    elif args.data == "PEMS08_60":
        args.data = "data/" + args.data
        args.num_nodes = 170
        args.input_len = 60
        args.output_len = 60
    elif args.data == "PEMS04_60":
        args.data = "data/" + args.data
        args.num_nodes = 307
        args.input_len = 60
        args.output_len = 60
    elif args.data == "PEMS03":
        args.data = "data/" + args.data
        args.num_nodes = 358
        args.epochs = 2000
        args.es_patience = 300
    elif args.data == "PEMS04":
        args.data = "data/" + args.data
        args.num_nodes = 307
    elif args.data == "PEMS07":
        args.data = "data/PEMS07"
        args.num_nodes = 883


    device = torch.device(args.device)

    # ä¿å­˜å®éªŒé…ç½®
    logger_mgr.save_config(args)
    logger.info(f"å®éªŒé…ç½®: {vars(args)}")


    dataloader = util.load_dataset(
        args.data, args.batch_size, args.batch_size, args.batch_size
        )
    scaler = dataloader["scaler"]

    path = logger_mgr.get_model_save_path() + "/"  # æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
    checkpoint_path = os.path.join(logger_mgr.get_experiment_dir(),
                                   "checkpoint.pth")  # æ£€æŸ¥ç‚¹è·¯å¾„

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    engine = trainer(
        scaler,
        args.input_dim,
        args.channels,
        args.num_nodes,
        args.input_len,
        args.output_len,
        args.dropout,
        args.learning_rate,
        args.weight_decay,
        device,
    )
    # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = ReduceLROnPlateau(
        engine.optimizer,
        mode='min',  # ç›‘æ§æŒ‡æ ‡æ˜¯è¶Šå°è¶Šå¥½
        factor=0.5,  # é™ä½å­¦ä¹ ç‡çš„å€æ•° (æ¯æ¬¡å‡å°‘ 50%)
        patience=60,  # è¿ç»­ 60 ä¸ª Epoch éªŒè¯æŸå¤±ä¸ä¸‹é™åˆ™è§¦å‘
        verbose=True,
        min_lr=1e-6  # æœ€ä½å­¦ä¹ ç‡é™åˆ¶
    )

    # --- æ£€æŸ¥ç‚¹åŠ è½½é€»è¾‘ ---
    # --- æ£€æŸ¥ç‚¹åŠ è½½é€»è¾‘ (å·²ä¿®æ­£) ---
    load_success, start_epoch_loaded, loss_loaded, test_log_loaded, epochs_since_best_mae_loaded = engine.load_checkpoint(
        checkpoint_path, logger)
    if load_success:
        start_epoch = start_epoch_loaded
        loss = loss_loaded
        test_log = test_log_loaded
        epochs_since_best_mae = epochs_since_best_mae_loaded
        bestid = start_epoch - 1
        try:
            checkpoint = torch.load(checkpoint_path)
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        except Exception as e:
            logger.warning(f"æœªèƒ½åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€: {e}")

    current_epoch = start_epoch  # å…³é”®ï¼šå¾ªç¯å¼€å§‹å‰è®¾ç½®

    # --- ä¿¡å·å¤„ç†å‡½æ•° ---
    def signal_handler(sig, frame):
        logger.warning("ğŸš¨ æ”¶åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)ã€‚æ­£åœ¨ä¿å­˜æ£€æŸ¥ç‚¹...")
        save_checkpoint(
            current_epoch,
            engine.model,
            engine.optimizer,
            scheduler,
            loss,
            test_log,
            epochs_since_best_mae,
            checkpoint_path,
            logger
        )
        logger.info("ç¨‹åºå®‰å…¨é€€å‡ºã€‚")
        sys.exit(0)

    # æ³¨å†Œä¿¡å·å¤„ç†å‡½æ•°
    signal.signal(signal.SIGINT, signal_handler)
    logger.info(f"å·²è®¾ç½® Ctrl+C (SIGINT) ä¿¡å·å¤„ç†ã€‚æ£€æŸ¥ç‚¹è·¯å¾„: {checkpoint_path}")

    # -------------------

    # è®°å½•æ¨¡å‹ä¿¡æ¯
    logger_mgr.log_model_info(str(engine.model))

    logger.info("å¼€å§‹è®­ç»ƒ...")

    his_loss = []
    val_time = []
    train_time = []
    result = []
    test_result = []

    # è®­ç»ƒå¾ªç¯ä» start_epoch å¼€å§‹
    for epoch in range(start_epoch, args.epochs + 1):
        current_epoch = epoch  # æ›´æ–°å…¨å±€å˜é‡
        should_save_checkpoint = False
        logger_mgr.log_epoch_start(epoch, args.epochs)

        # train
        train_loss = []
        train_mape = []
        train_rmse = []
        train_wmape = []

        t1 = time.time()
        dataloader['train_loader'].shuffle()
        for iter, (x, y) in enumerate(dataloader["train_loader"].get_iterator()):
            trainx = torch.Tensor(x).to(device)  # 64 12 170 1
            trainx = trainx.transpose(1, 3)
            trainy = torch.Tensor(y).to(device)
            trainy = trainy.transpose(1, 3)
            # trainy[:, 0, :, :] ä»…å–ç¬¬ä¸€ä¸ªç‰¹å¾
            metrics = engine.train(trainx, trainy[:, 0, :, :])
            train_loss.append(metrics[0])
            train_mape.append(metrics[1])
            train_rmse.append(metrics[2])
            train_wmape.append(metrics[3])

        t2 = time.time()
        train_time.append(t2 - t1)

        # validation
        valid_loss = []
        valid_mape = []
        valid_wmape = []
        valid_rmse = []

        s1 = time.time()
        for iter, (x, y) in enumerate(dataloader["val_loader"].get_iterator()):
            testx = torch.Tensor(x).to(device)
            testx = testx.transpose(1, 3)
            testy = torch.Tensor(y).to(device)
            testy = testy.transpose(1, 3)
            metrics = engine.eval(testx, testy[:, 0, :, :])
            valid_loss.append(metrics[0])
            valid_mape.append(metrics[1])
            valid_rmse.append(metrics[2])
            valid_wmape.append(metrics[3])

        s2 = time.time()
        val_time.append(s2 - s1)

        mtrain_loss = np.mean(train_loss)
        mtrain_mape = np.mean(train_mape)
        mtrain_wmape = np.mean(train_wmape)
        mtrain_rmse = np.mean(train_rmse)

        mvalid_loss = np.mean(valid_loss)
        mvalid_mape = np.mean(valid_mape)
        mvalid_wmape = np.mean(valid_wmape)
        mvalid_rmse = np.mean(valid_rmse)

        scheduler.step(mvalid_loss)

        his_loss.append(mvalid_loss)
        train_m = dict(
            train_loss=mtrain_loss,
            train_rmse=mtrain_rmse,
            train_mape=mtrain_mape,
            train_wmape=mtrain_wmape,
            valid_loss=mvalid_loss,
            valid_rmse=mvalid_rmse,
            valid_mape=mvalid_mape,
            valid_wmape=mvalid_wmape,
        )
        train_m = pd.Series(train_m)
        result.append(train_m)

        # è®°å½•epochç»“æŸæŒ‡æ ‡
        logger_mgr.log_epoch_end(
            epoch,
            {
                'loss': mtrain_loss,
                'rmse': mtrain_rmse,
                'mape': mtrain_mape,
                'wmape': mtrain_wmape,
            },
            {
                'loss': mvalid_loss,
                'rmse': mvalid_rmse,
                'mape': mvalid_mape,
                'wmape': mvalid_wmape,
            },
            train_time[-1],
            val_time[-1],
        )

        if mvalid_loss < loss:
            logger.info("###Update tasks appear###")
            if epoch < 100:
                loss = mvalid_loss
                torch.save(engine.model.state_dict(), path + "best_model.pth")
                bestid = epoch
                epochs_since_best_mae = 0
                logger_mgr.log_best_model(epoch, "Valid Loss", mvalid_loss)
                should_save_checkpoint = True

            elif epoch > 100:
                # === æµ‹è¯•é›†è¯„ä¼°é€»è¾‘ (ä¸å˜) ===
                outputs = []
                realy = torch.Tensor(dataloader["y_test"]).to(device)
                realy = realy.transpose(1, 3)[:, 0, :, :]

                for iter, (x, y) in enumerate(dataloader["test_loader"].get_iterator()):
                    testx = torch.Tensor(x).to(device)
                    testx = testx.transpose(1, 3)
                    with torch.no_grad():
                        preds = engine.model(testx).transpose(1, 3)
                    outputs.append(preds.squeeze())

                yhat = torch.cat(outputs, dim=0)
                yhat = yhat[: realy.size(0), ...]

                amae = []
                amape = []
                awmape = []
                armse = []

                # è®¡ç®—é€ horizon MAE
                for j in range(args.output_len):
                    pred = scaler.inverse_transform(yhat[:, :, j])
                    real = realy[:, :, j]
                    metrics = util.metric(pred, real)
                    amae.append(metrics[0])
                    amape.append(metrics[1])
                    armse.append(metrics[2])
                    awmape.append(metrics[3])

                avg_test_mae = np.mean(amae)

                logger.info(
                    f"Epoch {epoch} - Average Test MAE over {args.output_len} horizons: {avg_test_mae:.4f}")

                if avg_test_mae < test_log:
                    test_log = avg_test_mae
                    loss = mvalid_loss
                    torch.save(engine.model.state_dict(), path + "best_model.pth")
                    epochs_since_best_mae = 0
                    logger_mgr.log_best_model(epoch, "Test MAE (avg)", float(avg_test_mae))
                    bestid = epoch
                    logger.info(f"epoch: {epoch}")

                    # è®°å½•é€ horizon æŒ‡æ ‡
                    for j in range(args.output_len):
                        logger.info(
                            f"Evaluate best model on test data for horizon {j + 1}, Test MAE: {amae[j]:.4f}, Test RMSE: {armse[j]:.4f}, Test MAPE: {amape[j]:.4f}, Test WMAPE: {awmape[j]:.4f}"
                        )
                    should_save_checkpoint = True  # <--- æ ‡è®°ä¿å­˜æ£€æŸ¥ç‚¹
                else:
                    epochs_since_best_mae += 1
                    logger.info("No update in Test MAE")

        else:
            epochs_since_best_mae += 1
            logger.info("No update in Valid Loss")

        # === ç»Ÿä¸€æ£€æŸ¥ç‚¹ä¿å­˜é€»è¾‘ (å®ç°æ‚¨çš„è¦æ±‚) ===
        if should_save_checkpoint:
            save_checkpoint(
                current_epoch,
                engine.model,
                engine.optimizer,
                scheduler,
                loss,
                test_log,
                epochs_since_best_mae,
                checkpoint_path,
                logger
            )


        # === 200å‘¨æœŸåæ¯5ä¸ªå‘¨æœŸæµ‹è¯• ===
        # åŸæœ‰é€»è¾‘ï¼šå¦‚æœä¸æ˜¯æœ€ä½³æ¨¡å‹ï¼Œä½†æ»¡è¶³ i >= 50 å’Œ i % 5 == 0ï¼Œæ‰§è¡Œä¸€æ¬¡æµ‹è¯•é›†è¯„ä¼°å¹¶è®°å½•
        if epoch >= 50 and epoch % 5 == 0 and not (mvalid_loss < loss and epoch > 100):
            outputs = []
            realy = torch.Tensor(dataloader["y_test"]).to(device)
            realy = realy.transpose(1, 3)[:, 0, :, :]

            for iter, (x, y) in enumerate(dataloader["test_loader"].get_iterator()):
                testx = torch.Tensor(x).to(device)
                testx = testx.transpose(1, 3)
                with torch.no_grad():
                    preds = engine.model(testx).transpose(1, 3)
                outputs.append(preds.squeeze())

            yhat = torch.cat(outputs, dim=0)
            yhat = yhat[: realy.size(0), ...]

            amae = []
            for j in range(args.output_len):
                pred = scaler.inverse_transform(yhat[:, :, j])
                real = realy[:, :, j]
                metrics = util.metric(pred, real)
                amae.append(metrics[0])  # åªæ”¶é›†MAE

            avg_test_mae = np.mean(amae)
            logger.info(
                f"Epoch {epoch} - Average Test MAE over {args.output_len} horizons: {avg_test_mae:.4f}")

        # åŒæ­¥ä¿å­˜é€epochæŒ‡æ ‡å¿«ç…§
        train_csv = pd.DataFrame(result)
        train_csv.round(8).to_csv(
            f"{logger_mgr.get_experiment_dir()}/metrics/train.csv"
        )
        if epochs_since_best_mae >= args.es_patience and epoch >= 300:
            logger.info(f"æ—©åœè§¦å‘ï¼šè¿ç»­ {args.es_patience} ä¸ª Epoch æ€§èƒ½æœªæå‡ã€‚")
            break

    # Output consumption
    logger.info("Average Training Time: {:.4f} secs/epoch".format(np.mean(train_time)))
    logger.info("Average Inference Time: {:.4f} secs".format(np.mean(val_time)))

    # test
    logger.info("Training ends")
    logger.info("The epoch of the best resultï¼š%s" % bestid)
    # his_loss åˆ—è¡¨å¯èƒ½åœ¨åŠ è½½æ£€æŸ¥ç‚¹æ—¶æœ‰å†å²æ•°æ®ä¸¢å¤±ï¼Œè¿™é‡Œæœ€å¥½ä½¿ç”¨æ¢å¤çš„ loss å˜é‡
    logger.info(
        "The valid loss of the best model %s" % str(round(loss, 4))
    )

    # ... (æœ€ç»ˆæµ‹è¯•é€»è¾‘ä¸å˜)
    engine.model.load_state_dict(torch.load(path + "best_model.pth"))
    outputs = []
    realy = torch.Tensor(dataloader["y_test"]).to(device)
    realy = realy.transpose(1, 3)[:, 0, :, :]

    for iter, (x, y) in enumerate(dataloader["test_loader"].get_iterator()):
        testx = torch.Tensor(x).to(device)
        testx = testx.transpose(1, 3)
        with torch.no_grad():
            preds = engine.model(testx).transpose(1, 3)
        outputs.append(preds.squeeze())

    yhat = torch.cat(outputs, dim=0)
    yhat = yhat[: realy.size(0), ...]

    amae = []
    amape = []
    armse = []
    awmape = []

    test_m = []

    for i in range(args.output_len):
        pred = scaler.inverse_transform(yhat[:, :, i])
        real = realy[:, :, i]
        metrics = util.metric(pred, real)
        logger.info(
            f"Evaluate best model on test data for horizon {i + 1}, Test MAE: {metrics[0]:.4f}, Test RMSE: {metrics[2]:.4f}, Test MAPE: {metrics[1]:.4f}, Test WMAPE: {metrics[3]:.4f}"
        )

        test_m = dict(
            test_loss=np.mean(metrics[0]),
            test_rmse=np.mean(metrics[2]),
            test_mape=np.mean(metrics[1]),
            test_wmape=np.mean(metrics[3]),
        )
        test_m = pd.Series(test_m)
        test_result.append(test_m)

        amae.append(metrics[0])
        amape.append(metrics[1])
        armse.append(metrics[2])
        awmape.append(metrics[3])

    logger.info(
        f"On average over {args.output_len} horizons, Test MAE: {np.mean(amae):.4f}, Test RMSE: {np.mean(armse):.4f}, Test MAPE: {np.mean(amape):.4f}, Test WMAPE: {np.mean(awmape):.4f}"
    )

    test_m = dict(
        test_loss=np.mean(amae),
        test_rmse=np.mean(armse),
        test_mape=np.mean(amape),
        test_wmape=np.mean(awmape),
    )
    test_m = pd.Series(test_m)
    test_result.append(test_m)

    # è¾“å‡ºæµ‹è¯•ç»“æœè‡³æ—¥å¿—ç³»ç»Ÿä¸CSV
    horizon_metrics = []
    for idx in range(len(amae)):
        horizon_metrics.append({
            'mae': float(amae[idx]),
            'rmse': float(armse[idx]),
            'mape': float(amape[idx]),
            'wmape': float(awmape[idx])
        })
    overall_metrics = {
        'mae': float(np.mean(amae)),
        'rmse': float(np.mean(armse)),
        'mape': float(np.mean(amape)),
        'wmape': float(np.mean(awmape))
    }
    logger_mgr.log_test_results(overall_metrics, horizon_metrics)

    test_csv = pd.DataFrame(test_result)
    test_csv.round(8).to_csv(
        f"{logger_mgr.get_experiment_dir()}/metrics/test.csv"
    )


if __name__ == "__main__":
    t1 = time.time()
    main()
    t2 = time.time()
    # ç¨‹åºæ€»è€—æ—¶è®°å½•
    print("Total time spent: {:.4f}".format(t2 - t1))
